{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5029a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is alive again!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, regexp_extract, length, col, \n",
    "    avg, lower, regexp_replace, udf, split, element_at, desc\n",
    ")\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GutenbergFinal\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark is alive again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73204b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question 10: Final Results ---\n",
      "\n",
      "[A] Number of books released each year:\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|1975|    1|\n",
      "|1978|    1|\n",
      "|1979|    1|\n",
      "|1991|    6|\n",
      "|1992|   13|\n",
      "|1993|   12|\n",
      "|1994|   16|\n",
      "|1995|   59|\n",
      "|1996|   53|\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "[B] Language Analysis:\n",
      "The most common language is: English (Count: 162)\n",
      "\n",
      "[C] Title Length Analysis:\n",
      "The average length of book titles is: 23.75 characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"C:/Users/SUDHEESH/Downloads/D184MB/D184MB/*.txt\"\n",
    "\n",
    "raw_df = spark.read.text(dataset_path, wholetext=True)\n",
    "books_df = raw_df.withColumn(\"file_name\", input_file_name()) \\\n",
    "                 .withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# Extract Metadata\n",
    "extracted_df = books_df.withColumn(\"title\", regexp_extract(\"text\", r\"Title:\\s+(.*)\", 1)) \\\n",
    "    .withColumn(\"release_date\", regexp_extract(\"text\", r\"Release Date:\\s+([A-Za-z]+,?\\s+\\d{4})\", 1)) \\\n",
    "    .withColumn(\"year_str\", regexp_extract(\"release_date\", r\"(\\d{4})\", 1)) \\\n",
    "    .withColumn(\"language\", regexp_extract(\"text\", r\"Language:\\s+(\\w+)\", 1)) \\\n",
    "    .withColumn(\"author\", regexp_extract(\"text\", r\"Author:\\s+(.*)\", 1))\n",
    "\n",
    "\n",
    "extracted_df = extracted_df.selectExpr(\"*\", \"try_cast(year_str as int) as year\")\n",
    "\n",
    "clean_metadata_df = extracted_df.filter((col(\"year\").isNotNull()) & (col(\"year_str\") != \"\"))\n",
    "\n",
    "\n",
    "print(\"--- Question 10: Final Results ---\")\n",
    "\n",
    "print(\"\\n[A] Number of books released each year:\")\n",
    "books_per_year = clean_metadata_df.groupBy(\"year\").count().orderBy(\"year\")\n",
    "books_per_year.show(n=100)\n",
    "\n",
    "# Find the most common language\n",
    "print(\"\\n[B] Language Analysis:\")\n",
    "lang_df = clean_metadata_df.groupBy(\"language\").count().orderBy(\"count\", ascending=False)\n",
    "most_common = lang_df.first()\n",
    "print(f\"The most common language is: {most_common['language']} (Count: {most_common['count']})\")\n",
    "\n",
    "# Determine the average length of book titles\n",
    "print(\"\\n[C] Title Length Analysis:\")\n",
    "avg_val = clean_metadata_df.select(avg(length(\"title\"))).first()[0]\n",
    "print(f\"The average length of book titles is: {avg_val:.2f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03538440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 11: Top 5 Similar Books to 10.txt ---\n",
      "+---------+----------+\n",
      "|book_name|similarity|\n",
      "+---------+----------+\n",
      "|   30.txt|       1.0|\n",
      "|   58.txt|    0.4765|\n",
      "|   26.txt|      0.45|\n",
      "|  169.txt|    0.4257|\n",
      "|  357.txt|    0.3613|\n",
      "+---------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\n",
    "from pyspark.sql.functions import desc, round\n",
    "\n",
    "clean_text_df = extracted_df.withColumn(\"text_clean\", lower(col(\"text\"))) \\\n",
    "                            .withColumn(\"text_clean\", regexp_replace(\"text_clean\", r\"[^a-z\\s]\", \"\"))\n",
    "\n",
    "# Vectorization Pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\") \n",
    "\n",
    "\n",
    "tfidf_df = idf.fit(hashingTF.transform(remover.transform(tokenizer.transform(clean_text_df)))).transform(\n",
    "    hashingTF.transform(remover.transform(tokenizer.transform(clean_text_df)))\n",
    ")\n",
    "normalized_df = normalizer.transform(tfidf_df).select(\"file_name\", \"normFeatures\").cache()\n",
    "\n",
    "# Similarity for '10.txt'\n",
    "target_file = \"10.txt\"\n",
    "target_row = normalized_df.filter(col(\"file_name\").contains(target_file)).select(\"normFeatures\").first()\n",
    "\n",
    "if target_row:\n",
    "    target_vec = target_row[0]\n",
    "    sim_udf = udf(lambda v: float(v.dot(target_vec)))\n",
    "    \n",
    "results = normalized_df.withColumn(\"similarity\", sim_udf(col(\"normFeatures\"))) \\\n",
    "    .filter(~col(\"file_name\").contains(target_file)) \\\n",
    "    .withColumn(\"book_name\", element_at(split(col(\"file_name\"), \"/\"), -1)) \\\n",
    "    .select(\"book_name\", round(col(\"similarity\"), 4).alias(\"similarity\")) \\\n",
    "    .orderBy(desc(\"similarity\"))\n",
    "    \n",
    "print(f\"\\n--- Question 11: Top 5 Similar Books to {target_file} ---\")\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d15c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 12: Refined Influence Network ---\n",
      "Top 5 Influencers (Out-Degree):\n",
      "+--------------------+-----+\n",
      "|          influencer|count|\n",
      "+--------------------+-----+\n",
      "|    G. K. Chesterton|   86|\n",
      "|        Thomas Hardy|   58|\n",
      "|         John Milton|   57|\n",
      "|Edgar Rice Burroughs|   53|\n",
      "|Electronic Fronti...|   50|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "Top 5 Influenced (In-Degree):\n",
      "+--------------------+-----+\n",
      "|          influenced|count|\n",
      "+--------------------+-----+\n",
      "|        Thomas Hardy|   63|\n",
      "|Mary Roberts Rine...|   61|\n",
      "|         Henry James|   61|\n",
      "|       Joseph Conrad|   61|\n",
      "|  Arthur Conan Doyle|   61|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "author_data = clean_metadata_df.select(\"author\", \"year\", \"language\").distinct().filter(\"author != ''\")\n",
    "\n",
    "influence_edges = author_data.alias(\"a\").join(\n",
    "    author_data.alias(\"b\"),\n",
    "    (col(\"b.year\") > col(\"a.year\")) & \n",
    "    (col(\"b.year\") <= col(\"a.year\") + 1) & \n",
    "    (col(\"a.author\") != col(\"b.author\")) &\n",
    "    (col(\"a.language\") == col(\"b.language\"))\n",
    ").select(col(\"a.author\").alias(\"influencer\"), col(\"b.author\").alias(\"influenced\")).distinct()\n",
    "\n",
    "print(\"\\n--- Question 12: Refined Influence Network ---\")\n",
    "print(\"Top 5 Influencers (Out-Degree):\")\n",
    "influence_edges.groupBy(\"influencer\").count().orderBy(desc(\"count\")).show(5)\n",
    "\n",
    "print(\"Top 5 Influenced (In-Degree):\")\n",
    "influence_edges.groupBy(\"influenced\").count().orderBy(desc(\"count\")).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
